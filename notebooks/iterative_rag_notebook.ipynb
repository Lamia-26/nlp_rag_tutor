{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2cd4f5",
   "metadata": {},
   "source": [
    "# NLP RAG Tutor — Démarche itérative (Notebook)\n",
    "\n",
    "Ce notebook illustre **la démarche itérative** suivie pour améliorer un système **RAG (Retrieval-Augmented Generation)** à partir des pdfs\n",
    "\n",
    "On montre :\n",
    "1. **Baseline** (chunking + embeddings + retrieval)\n",
    "2. **Itération 1 — Chunking** (taille/overlap) et impact sur Recall@k / MRR\n",
    "3. **Itération 2 — Embeddings** (multilingue vs anglais) et impact\n",
    "4. **Itération 3 — Prompt** (anti-hallucination) et exemple qualitatif\n",
    "5. **Itération 4 — small_to_big** \n",
    "\n",
    "5. **Conclusion** : choix final\n",
    "\n",
    "> Prérequis : avoir généré  `data/interim/pages.jsonl` via `python -m src.main ingest`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9331ca",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adf454f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to PYTHONPATH: c:\\Users\\0204528N\\Desktop\\Projet_nlp\\nlp-rag-tutor\n",
      "OK: pages.jsonl et questions.csv trouvés.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "\n",
    "if not (ROOT / \"src\").exists():\n",
    "    raise RuntimeError(\"Structure du projet invalide : dossier src introuvable\")\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"Project root added to PYTHONPATH:\", ROOT)\n",
    "\n",
    "PAGES = ROOT / \"data/interim/pages.jsonl\"\n",
    "QUESTIONS = ROOT / \"data/eval/questions.csv\"\n",
    "assert PAGES.exists(), \"Fichier pages.jsonl introuvable. Lance: python -m src.main ingest\"\n",
    "assert QUESTIONS.exists(), \"questions.csv introuvable. Crée: data/eval/questions.csv\"\n",
    "print(\"OK: pages.jsonl et questions.csv trouvés.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9135794",
   "metadata": {},
   "source": [
    "## 1) Fonctions utilitaires : index + évaluation\n",
    "\n",
    "On va :\n",
    "- chunker (max_chars, overlap_chars)\n",
    "- indexer avec FAISS (embed_model)\n",
    "- évaluer le retrieval (Recall@k, MRR) via la commande `run_evaluation()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52287932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "from src.utils.io import read_jsonl, write_jsonl\n",
    "from src.chunking.chunker import chunk_pages, ChunkConfig\n",
    "from src.retrieval.build_index import build_index\n",
    "from src.retrieval.embedder import EmbeddingConfig\n",
    "from src.eval.evaluate import run_evaluation, EvalConfig\n",
    "\n",
    "def build_run(\n",
    "    run_name: str,\n",
    "    max_chars: int,\n",
    "    overlap_chars: int,\n",
    "    top_k: int,\n",
    "    embed_model: str,\n",
    ") -> Dict:\n",
    "    run_dir = Path(\"data/experiments\") / run_name\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Chunk\n",
    "    pages = list(read_jsonl(PAGES))\n",
    "\n",
    "    cfg = ChunkConfig(max_chars=max_chars, overlap_chars=overlap_chars, min_chars=300)\n",
    "    chunks = chunk_pages(pages, cfg)\n",
    "    chunks_path = run_dir / \"chunks.jsonl\"\n",
    "    write_jsonl(chunks_path, chunks)\n",
    "\n",
    "    # 2) Index\n",
    "    index_dir = run_dir / \"faiss\"\n",
    "    if index_dir.exists():\n",
    "        shutil.rmtree(index_dir)\n",
    "    build_index(\n",
    "        chunks_jsonl=chunks_path,\n",
    "        index_dir=index_dir,\n",
    "        embed_cfg=EmbeddingConfig(model_name=embed_model, normalize=True),\n",
    "    )\n",
    "\n",
    "    # 3) Evaluate retrieval\n",
    "    metrics = run_evaluation(\n",
    "        index_dir=index_dir,\n",
    "        cfg=EvalConfig(\n",
    "            questions_csv=QUESTIONS,\n",
    "            out_dir=run_dir / \"eval\",\n",
    "            top_k=top_k,\n",
    "            embed_model=embed_model,\n",
    "            use_llm=False,\n",
    "        ),\n",
    "    )\n",
    "    metrics.update({\n",
    "        \"run_name\": run_name,\n",
    "        \"max_chars\": max_chars,\n",
    "        \"overlap_chars\": overlap_chars,\n",
    "        \"top_k\": top_k,\n",
    "        \"embed_model\": embed_model,\n",
    "        \"n_chunks\": len(chunks),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "print(\"Ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7db649",
   "metadata": {},
   "source": [
    "## 2) Baseline\n",
    "\n",
    "- Chunking: `max_chars=3500`, `overlap=400` \n",
    "- Embeddings: multilingue (`paraphrase-multilingual-MiniLM-L12-v2`)\n",
    "- top_k: 8\n",
    "\n",
    "On mesure Recall@8 et MRR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e7b4226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 22/22 [00:42<00:00,  1.92s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_questions': 40,\n",
       " 'recall@8': 0.825,\n",
       " 'mrr': 0.6320833333333333,\n",
       " 'run_name': 'baseline_multilingual_3500_400',\n",
       " 'max_chars': 3500,\n",
       " 'overlap_chars': 400,\n",
       " 'top_k': 8,\n",
       " 'embed_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       " 'n_chunks': 688,\n",
       " 'run_dir': 'data\\\\experiments\\\\baseline_multilingual_3500_400'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = build_run(\n",
    "    run_name=\"baseline_multilingual_3500_400\",\n",
    "    max_chars=3500,\n",
    "    overlap_chars=400,\n",
    "    top_k=8,\n",
    "    embed_model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    ")\n",
    "baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5baa322",
   "metadata": {},
   "source": [
    "## 3) Itération 1 — Chunking \n",
    "\n",
    "Avec un livre long et dense, des chunks plus petits donnent souvent un meilleur retrieval :\n",
    "- `max_chars=2500`, `overlap=300`\n",
    "\n",
    "On garde le même modèle d'embeddings (multilingue) pour isoler l'effet du chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e83118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:40<00:00,  1.32s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_questions': 40,\n",
       " 'recall@8': 0.875,\n",
       " 'mrr': 0.7098214285714286,\n",
       " 'run_name': 'iter1_chunking_multilingual_2500_300',\n",
       " 'max_chars': 2500,\n",
       " 'overlap_chars': 300,\n",
       " 'top_k': 8,\n",
       " 'embed_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       " 'n_chunks': 962,\n",
       " 'run_dir': 'data\\\\experiments\\\\iter1_chunking_multilingual_2500_300'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_iter = build_run(\n",
    "    run_name=\"iter1_chunking_multilingual_2500_300\",\n",
    "    max_chars=2500,\n",
    "    overlap_chars=300,\n",
    "    top_k=8,\n",
    "    embed_model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    ")\n",
    "chunk_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12adc7af",
   "metadata": {},
   "source": [
    "## 4) Itération 2 — Embeddings (anglais vs multilingue)\n",
    "\n",
    "Le livre source est en **anglais**. Souvent, un modèle d'embeddings **anglais** améliore la similarité sémantique.\n",
    "On compare :\n",
    "- multilingue (`paraphrase-multilingual-MiniLM-L12-v2`)\n",
    "- anglais (`all-MiniLM-L6-v2`)\n",
    "\n",
    "On garde le chunking optimisé du livre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29d66a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:35<00:00,  1.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 97.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 65.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 79.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 78.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 79.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 74.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 74.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_questions': 40,\n",
       " 'recall@8': 0.95,\n",
       " 'mrr': 0.8800000000000001,\n",
       " 'run_name': 'iter2_embeddings_english_2500_300',\n",
       " 'max_chars': 2500,\n",
       " 'overlap_chars': 300,\n",
       " 'top_k': 8,\n",
       " 'embed_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'n_chunks': 962,\n",
       " 'run_dir': 'data\\\\experiments\\\\iter2_embeddings_english_2500_300'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_iter = build_run(\n",
    "    run_name=\"iter2_embeddings_english_2500_300\",\n",
    "    max_chars=2500,\n",
    "    overlap_chars=300,\n",
    "    top_k=8,\n",
    "    embed_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")\n",
    "embed_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc815bee",
   "metadata": {},
   "source": [
    "## 5) Résumé quantitatif des itérations\n",
    "\n",
    "On compare Recall@8 et MRR pour les 3 runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f39fd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>n_chunks</th>\n",
       "      <th>max_chars</th>\n",
       "      <th>overlap_chars</th>\n",
       "      <th>embed_model</th>\n",
       "      <th>recall@8</th>\n",
       "      <th>mrr</th>\n",
       "      <th>run_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline_multilingual_3500_400</td>\n",
       "      <td>688</td>\n",
       "      <td>3500</td>\n",
       "      <td>400</td>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-...</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.632083</td>\n",
       "      <td>data\\experiments\\baseline_multilingual_3500_400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iter1_chunking_multilingual_2500_300</td>\n",
       "      <td>962</td>\n",
       "      <td>2500</td>\n",
       "      <td>300</td>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.709821</td>\n",
       "      <td>data\\experiments\\iter1_chunking_multilingual_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iter2_embeddings_english_2500_300</td>\n",
       "      <td>962</td>\n",
       "      <td>2500</td>\n",
       "      <td>300</td>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>data\\experiments\\iter2_embeddings_english_2500...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               run_name  n_chunks  max_chars  overlap_chars  \\\n",
       "0        baseline_multilingual_3500_400       688       3500            400   \n",
       "1  iter1_chunking_multilingual_2500_300       962       2500            300   \n",
       "2     iter2_embeddings_english_2500_300       962       2500            300   \n",
       "\n",
       "                                         embed_model  recall@8       mrr  \\\n",
       "0  sentence-transformers/paraphrase-multilingual-...     0.825  0.632083   \n",
       "1  sentence-transformers/paraphrase-multilingual-...     0.875  0.709821   \n",
       "2             sentence-transformers/all-MiniLM-L6-v2     0.950  0.880000   \n",
       "\n",
       "                                             run_dir  \n",
       "0    data\\experiments\\baseline_multilingual_3500_400  \n",
       "1  data\\experiments\\iter1_chunking_multilingual_2...  \n",
       "2  data\\experiments\\iter2_embeddings_english_2500...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([baseline, chunk_iter, embed_iter])[\n",
    "    [\"run_name\",\"n_chunks\",\"max_chars\",\"overlap_chars\",\"embed_model\",\"recall@8\",\"mrr\",\"run_dir\"]\n",
    "]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848deee4",
   "metadata": {},
   "source": [
    "## 6) Itération 3 — Prompt (anti-hallucination) : démonstration qualitative\n",
    "\n",
    "Le retrieval peut être bon, mais la génération peut **inventer** une formule si on n'impose pas :\n",
    "- \"recopie la formule **mot pour mot** depuis les sources\"\n",
    "- \"si la formule n'est pas dans les extraits, le dire explicitement\"\n",
    "\n",
    "On compare 2 prompts :\n",
    "- Prompt “souple”\n",
    "- Prompt “strict” (anti-hallucination)\n",
    "\n",
    "> Cette section est qualitative : on montre l'effet sur une question sensible (TF-IDF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf0ced75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top source pages: [(121, 121, 0.6782), (301, 302, 0.6234), (292, 293, 0.6207)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval.retriever import Retriever, RetrieverConfig\n",
    "from src.rag.llm_groq import GroqLLM, GroqConfig\n",
    "\n",
    "# Choisir le meilleur index (anglais) pour la démo\n",
    "INDEX_DIR = Path(embed_iter[\"run_dir\"]) / \"faiss\"\n",
    "\n",
    "retriever = Retriever(\n",
    "    index_dir=INDEX_DIR,\n",
    "    embed_cfg=EmbeddingConfig(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", normalize=True),\n",
    "    cfg=RetrieverConfig(top_k=8),\n",
    ")\n",
    "\n",
    "q = \"Give the TF-IDF formula, then explain it simply.\"\n",
    "hits = retriever.retrieve(q)\n",
    "print(\"Top source pages:\", [(h[\"page_start\"], h[\"page_end\"], round(h[\"score\"],4)) for h in hits[:3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a60c9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_messages(question: str, hits, strict: bool) -> list[dict]:\n",
    "    sources = []\n",
    "    for i, h in enumerate(hits, start=1):\n",
    "        excerpt = (h.get(\"text\") or \"\")[:1200]\n",
    "        sources.append(\n",
    "            f\"[SOURCE {i}] pdf={h.get('pdf_name')} pages={h.get('page_start')}-{h.get('page_end')} score={h.get('score'):.4f}\\n{excerpt}\\n\"\n",
    "        )\n",
    "    sources_txt = \"\\n\".join(sources)\n",
    "\n",
    "    if strict:\n",
    "        system = (\n",
    "            \"You are an NLP tutor.\\n\"\n",
    "            \"STRICT RULES:\\n\"\n",
    "            \"1) Use ONLY the provided sources.\\n\"\n",
    "            \"2) If you write any formula (TF, IDF, TF-IDF), copy it EXACTLY from the sources  .\\n\"\n",
    "            \"3) If the exact formula is not present in the excerpts, say so and only explain the intuition.\\n\"\n",
    "            \"4) Cite sources like (SOURCE k, pdf, pages) \\n\"\n",
    "        )\n",
    "    else:\n",
    "        system = (\n",
    "            \"You are an NLP tutor. Use the sources , make it clean ,  to answer and cite them.\"\n",
    "        )\n",
    "\n",
    "    user = f\"Question:\\n{question}\\n\\nSOURCES:\\n{sources_txt}\\n\\nAnswer:\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "\n",
    "\n",
    "llm = GroqLLM(GroqConfig(model=\"llama-3.1-8b-instant\", temperature=0.2, max_tokens=450))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf59d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt souple ===\n",
      "\n",
      "**TF-IDF Formula:**\n",
      "\n",
      "The TF-IDF formula is used to calculate the importance of a word in a document. It is defined as the product of two components:\n",
      "\n",
      "1. **Term Frequency (TF)**: This measures the frequency of a word in a document. It is calculated as:\n",
      "\n",
      "TF = (1 + log(f)) / (1 + log(N))\n",
      "\n",
      "where f is the frequency of the word in the document and N is the total number of words in the document.\n",
      "\n",
      "2. **Inverse Document Frequency (IDF)**: This measures the rarity of a word across all documents. It is calculated as:\n",
      "\n",
      "IDF = log(N / df)\n",
      "\n",
      "where N is the total number of documents and df is the number of documents containing the word.\n",
      "\n",
      "The TF-IDF formula is then calculated as:\n",
      "\n",
      "TF-IDF = TF × IDF\n",
      "\n",
      "**Simplified Explanation:**\n",
      "\n",
      "Imagine you have a large collection of documents, and you want to find the most important words in each document. TF-IDF helps you do this by calculating the importance of each word based on two factors:\n",
      "\n",
      "1. **How often does the word appear in the document?** (Term Frequency)\n",
      "2. **How rare is the word across all documents?** (Inverse Document Frequency)\n",
      "\n",
      "The more often a word appears in a document, and the rarer it is across all documents, the more important it is. This is why TF-IDF is a powerful tool for information retrieval and text analysis.\n",
      "\n",
      "**Sources:**\n",
      "\n",
      "* [Sparck Jones, 1972] Sparck Jones, K. (1972). A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1), 11-21.\n",
      "* [Fano, 1961] Fano, R. M. (1961). Transmission of information: A statistical theory of communications. MIT Press.\n",
      "* [Cordier, 1965] Cordier, A. (1965). Factor analysis of word association probabilities. Journal of Experimental Psychology, 70(2), 147-153.\n",
      "* [Jurafsky, 2014] Jurafsky, D. (2014). Language models and the statistical analysis of language.\n",
      "\n",
      "Tokens: {'prompt_tokens': 1872, 'completion_tokens': 450, 'total_tokens': 2322}\n",
      "\n",
      "\n",
      "=================================================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "=== Prompt strict ===\n",
      "\n",
      "The TF-IDF formula is:\n",
      "\n",
      "N\n",
      "idf t = log 10 df (6.13)\n",
      "(cid:18) t (cid:19)\n",
      "\n",
      "where N is the total number of documents in the collection, and df is the number of documents in which term t occurs.\n",
      "\n",
      "The TF-IDF score for a term in a document is calculated as:\n",
      "\n",
      "tf-idf(t,q) tf-idf(t,d)\n",
      "score(q,d) = (14.9)\n",
      "(cid:88)t ∈q qi∈qtf-idf2(q i,q) · di∈dtf-idf2(d i,d)\n",
      "(cid:113) (cid:113)\n",
      "(cid:80) (cid:80)\n",
      "\n",
      "where tf-idf(t,q) is the TF-IDF score of term t in the query q, and tf-idf(t,d) is the TF-IDF score of term t in document d.\n",
      "\n",
      "Now, let's explain it simply:\n",
      "\n",
      "**TF-IDF** is a way to measure the importance of a word in a document. It's a combination of two things:\n",
      "\n",
      "1. **Term Frequency (TF)**: This measures how often a word appears in a document. If a word appears many times, it's considered more important.\n",
      "2. **Inverse Document Frequency (IDF)**: This measures how rare a word is in the entire collection of documents. If a word is rare, it's considered more important.\n",
      "\n",
      "The TF-IDF score is calculated by multiplying the TF and IDF scores. The result is a number that represents how important a word is in a document.\n",
      "\n",
      "In the formula, the IDF score is calculated as the logarithm of the number of documents divided by the number of documents that contain the term. This means that if a term appears in many documents, its IDF score will be low, and if it appears in few documents, its IDF score will be high.\n",
      "\n",
      "The TF-IDF score is then used to calculate the similarity between a query and a document. The similarity is measured as the cosine of the TF-IDF vectors of the query and document. The cosine is a measure of how similar two vectors are, and it's used to rank the documents in order of relevance to the query.\n",
      "\n",
      "Tokens: {'prompt_tokens': 1931, 'completion_tokens': 448, 'total_tokens': 2379}\n"
     ]
    }
   ],
   "source": [
    "messages_soft = build_messages(q, hits, strict=False)\n",
    "ans_soft, usage_soft = llm.chat(messages_soft)\n",
    "\n",
    "\n",
    "messages_strict = build_messages(q, hits, strict=True)\n",
    "ans_strict, usage_strict = llm.chat(messages_strict)\n",
    "\n",
    "print(\"=== Prompt souple ===\\n\")\n",
    "print(ans_soft)\n",
    "print(\"\\nTokens:\", usage_soft)\n",
    "print(\"\\n\\n=================================================================================================================================\\n\")\n",
    "\n",
    "print(\"\\n\\n=== Prompt strict ===\\n\")\n",
    "print(ans_strict)\n",
    "print(\"\\nTokens:\", usage_strict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9fc21",
   "metadata": {},
   "source": [
    "### jouer sur la maniere de formuler le prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e89134",
   "metadata": {},
   "source": [
    "**On observe un trade-off : un prompt souple produit une réponse pédagogique mais moins strictement ancrée dans les sources, tandis qu’un prompt strict peut dégrader la qualité lorsque les sources contiennent des artefacts d’extraction PDF (ex: (cid:...)).”**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b60b97",
   "metadata": {},
   "source": [
    "# Iteration 4: small-to-big "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "983e9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PAGES_DATA = list(read_jsonl(PAGES))\n",
    "\n",
    "page_lookup = {}\n",
    "for p in PAGES_DATA:\n",
    "    pdf = p.get(\"pdf_name\") or p.get(\"pdf\") or p.get(\"source\")\n",
    "    page = p.get(\"page\")\n",
    "    # IMPORTANT: ton fichier a \"text_raw\"\n",
    "    text = p.get(\"text_raw\") or p.get(\"text\") or \"\"\n",
    "\n",
    "    if pdf is None or page is None:\n",
    "        continue\n",
    "\n",
    "    page_lookup[(Path(str(pdf)).name, int(page))] = text\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_small2big(question: str, retriever, *, expand_pages: int = 1, top_k_small: int = 8):\n",
    "    \"\"\"\n",
    "    1) retrieve sur index small (chunks)\n",
    "    2) expansion en contexte big: concat des pages voisines autour des hits\n",
    "    \"\"\"\n",
    "    # 1) Small retrieve\n",
    "    hits = retriever.retrieve(question)[:top_k_small]\n",
    "\n",
    "    expanded = []\n",
    "    seen = set()\n",
    "\n",
    "    for h in hits:\n",
    "        pdf = h.get(\"pdf_name\")\n",
    "        ps = int(h.get(\"page_start\"))\n",
    "        pe = int(h.get(\"page_end\"))\n",
    "\n",
    "        # élargit la fenêtre\n",
    "        start = max(1, ps - expand_pages)\n",
    "        end = pe + expand_pages\n",
    "\n",
    "        key = (pdf, start, end)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        parts = []\n",
    "        for page in range(start, end + 1):\n",
    "            t = page_lookup.get((pdf, page))\n",
    "            if t:\n",
    "                parts.append(t)\n",
    "\n",
    "        big_text = \"\\n\\n\".join(parts).strip()\n",
    "        if not big_text:\n",
    "            continue\n",
    "\n",
    "        expanded.append({\n",
    "            \"pdf_name\": pdf,\n",
    "            \"page_start\": start,\n",
    "            \"page_end\": end,\n",
    "            \"score\": float(h.get(\"score\", 0.0)),\n",
    "            \"text\": big_text,\n",
    "            \"seed_chunk_pages\": (ps, pe),\n",
    "        })\n",
    "\n",
    "    # tri par score desc\n",
    "    expanded.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return hits, expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eb241f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SMALL HITS ===\n",
      "[(121, 121, 0.6782), (301, 302, 0.6234), (292, 293, 0.6207)]\n",
      "\n",
      "=== BIG (EXPANDED) HITS ===\n",
      "[(120, 122, 0.6782), (300, 303, 0.6234), (291, 294, 0.6207)]\n"
     ]
    }
   ],
   "source": [
    "q = \"Give the TF-IDF formula, then explain it simply.\"\n",
    "\n",
    "hits_small, hits_big = retrieve_small2big(\n",
    "    q,\n",
    "    retriever,\n",
    "    expand_pages=1,      \n",
    "    top_k_small=8\n",
    ")\n",
    "\n",
    "print(\"=== SMALL HITS ===\")\n",
    "print([(h[\"page_start\"], h[\"page_end\"], round(h[\"score\"], 4)) for h in hits_small[:3]])\n",
    "\n",
    "print(\"\\n=== BIG (EXPANDED) HITS ===\")\n",
    "print([(h[\"page_start\"], h[\"page_end\"], round(h[\"score\"], 4)) for h in hits_big[:3]])\n",
    "\n",
    "messages_soft = build_messages(q, hits_big, strict=False)\n",
    "ans_soft, usage_soft = llm.chat(messages_soft)\n",
    "\n",
    "messages_strict = build_messages(q, hits_big, strict=True)\n",
    "ans_strict, usage_strict = llm.chat(messages_strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0ea333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== expand_pages=0 ===\n",
      "small: [(121, 121), (301, 302), (292, 293)]\n",
      "big  : [(121, 121), (301, 302), (292, 293)]\n",
      "answer snippet: The TF-IDF formula is not explicitly provided in the sources. However, we can infer the formula from the given information.\n",
      "\n",
      "The TF-IDF formula is a product of two components:\n",
      "\n",
      "1. Term Frequency (TF): This measures the frequency of a term in a document.\n",
      "2. Inverse Document Frequency (IDF): This meas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== expand_pages=1 ===\n",
      "small: [(121, 121), (301, 302), (292, 293)]\n",
      "big  : [(120, 122), (300, 303), (291, 294)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer snippet: The TF-IDF formula is not explicitly provided in the sources. However, we can infer the formula from the information given in the sources.\n",
      "\n",
      "The TF-IDF formula is a combination of two weights:\n",
      "\n",
      "1. Term Frequency (TF): This weight measures the importance of a term in a document. It is calculated as th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== expand_pages=2 ===\n",
      "small: [(121, 121), (301, 302), (292, 293)]\n",
      "big  : [(119, 123), (299, 304), (290, 295)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer snippet: The TF-IDF formula is not explicitly provided in the sources. However, we can infer the TF-IDF formula from SOURCE 4, which provides an example of a tf-idf weighted term-document matrix.\n",
      "\n",
      "From SOURCE 4, we can see that the value for the word \"wit\" in the play \"As You Like It\" is 0.085, which is the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== expand_pages=3 ===\n",
      "small: [(121, 121), (301, 302), (292, 293)]\n",
      "big  : [(118, 124), (298, 305), (289, 296)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer snippet: The TF-IDF formula is:\n",
      "\n",
      "idf = log(N/df) (SOURCE 4, pdf, pages 121-128)\n",
      "\n",
      "where N is the total number of documents in the collection, and df is the number of documents in which term t occurs.\n",
      "\n",
      "The TF-IDF formula is the product of two weights:\n",
      "\n",
      "tf-idf = tf * idf\n",
      "\n",
      "where tf is the term frequency, which i\n"
     ]
    }
   ],
   "source": [
    "q = \"Give the TF-IDF formula, then explain it simply.\"\n",
    "\n",
    "for e in [0, 1, 2, 3]:\n",
    "    hits_small, hits_big = retrieve_small2big(q, retriever, expand_pages=e, top_k_small=8)\n",
    "    print(f\"\\n=== expand_pages={e} ===\")\n",
    "    print(\"small:\", [(h[\"page_start\"], h[\"page_end\"]) for h in hits_small[:3]])\n",
    "    print(\"big  :\", [(h[\"page_start\"], h[\"page_end\"]) for h in hits_big[:3]])\n",
    "\n",
    "    messages = build_messages(q, hits_big, strict=True)\n",
    "    ans, usage = llm.chat(messages)\n",
    "    print(\"answer snippet:\", ans[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7121ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dans cette iteration , nous avons étudié une stratégie de récupération *small-to-big*  \n",
    "L’idée consiste à récupérer d’abord des passages courts et précis, puis à élargir progressivement le contexte en incluant les pages voisines.\n",
    "\n",
    "Les résultats montrent que l’utilisation de petits passages seuls n’est souvent pas suffisante pour répondre correctement à des questions nécessitant des définitions complètes ou des formules.  \n",
    "En élargissant le contexte autour des passages pertinents, le modèle a accès à davantage d’informations utiles, ce qui améliore la complétude et la qualité des réponses.\n",
    "\n",
    "Cette expérience met en évidence l’intérêt de la stratégie *small-to-big*, qui permet de trouver un bon compromis entre précision du retrieval et richesse du contexte dans les systèmes RAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157d28e",
   "metadata": {},
   "source": [
    "## 7) Conclusion\n",
    "\n",
    "- **Chunking adapté livre** améliore souvent la récupération\n",
    "- **Embeddings anglais** améliorent nettement la similarité sur un corpus anglais\n",
    "- **Prompt strict** réduit les hallucinations et force les formules exactes\n",
    "\n",
    "Le pipeline final (retenu) :\n",
    "- `max_chars=2500`, `overlap=300`\n",
    "- embeddings: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- top_k: 8 (ou 10 sur questions difficiles)\n",
    "- prompt anti-hallucination\n",
    "-  Small → Big retrieval permet de compléter l’information en élargissant le contexte autour des chunks pertinents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3bae11",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------Merci"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-rag-venv)",
   "language": "python",
   "name": "nlp-rag-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
